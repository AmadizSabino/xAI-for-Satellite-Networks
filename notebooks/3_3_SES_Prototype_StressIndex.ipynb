{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPeFscF7q2yivZHfM1cCeC7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IX2ZEvBHWmIB","executionInfo":{"status":"ok","timestamp":1764516473720,"user_tz":0,"elapsed":4595,"user":{"displayName":"Amadiz Sabino","userId":"07507222221399686296"}},"outputId":"8ac2c073-3ae6-442b-f96a-2353171bbcfb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","BASE_DIR    : /content/drive/MyDrive/Colab_Notebooks/Thesis-AI/phase3_ses\n","ART_CAP     : /content/drive/MyDrive/Colab_Notebooks/Thesis-AI/phase3_ses/artifacts_capacity\n","ART_SLA     : /content/drive/MyDrive/Colab_Notebooks/Thesis-AI/phase3_ses/artifacts_sla\n","ART_SIG     : /content/drive/MyDrive/Colab_Notebooks/Thesis-AI/phase3_ses/artifacts_signal_loss\n","ART_HAND    : /content/drive/MyDrive/Colab_Notebooks/Thesis-AI/phase3_ses/artifacts_handover\n","ART_STRESS  : /content/drive/MyDrive/Colab_Notebooks/Thesis-AI/phase3_ses/artifacts_stress\n","Loading: /content/drive/MyDrive/Colab_Notebooks/Thesis-AI/phase3_ses/artifacts_capacity/capacity_risk_timeseries.csv\n","Capacity risk shape: (25168, 12)\n","                        time beam_id  demand_mbps  capacity_mbps  demand_norm  \\\n","2  2021-10-18 07:20:00+00:00  AFRICA   155.229844            700     0.221757   \n","6  2021-10-18 07:30:00+00:00  AFRICA   125.468461            700     0.179241   \n","10 2021-10-18 07:40:00+00:00  AFRICA   143.945750            700     0.205637   \n","14 2021-10-18 07:50:00+00:00  AFRICA   121.860099            700     0.174086   \n","18 2021-10-18 08:00:00+00:00  AFRICA   128.745761            700     0.183923   \n","\n","    jamming_risk  sla_risk  signal_loss_risk  handover_risk  \\\n","2       0.298021       0.0               0.0            0.0   \n","6       0.185325       0.0               0.0            0.0   \n","10      0.144313       0.0               0.0            0.0   \n","14      0.184565       0.0               0.0            0.0   \n","18      0.207531       0.0               0.0            0.0   \n","\n","    space_weather_risk  risk_index  capacity_incident  \n","2             0.034222    0.101833                  0  \n","6             0.029479    0.073106                  0  \n","10            0.028984    0.079499                  0  \n","14            0.010073    0.069030                  0  \n","18            0.008313    0.075080                  0  \n","Beams in capacity dataset: ['AFRICA' 'AMERICAS' 'ASIA' 'EUROPE']\n","Loading: /content/drive/MyDrive/Colab_Notebooks/Thesis-AI/phase3_ses/ses_comm_anomalies.csv\n","Loading: /content/drive/MyDrive/Colab_Notebooks/Thesis-AI/phase3_ses/artifacts_sla/sla_breach_events.csv\n","[WARN] Missing file: /content/drive/MyDrive/Colab_Notebooks/Thesis-AI/phase3_ses/artifacts_signal_loss/signal_loss_events.csv\n","[INFO] No signal-loss events found – loss_risk = 0.\n","Loading: /content/drive/MyDrive/Colab_Notebooks/Thesis-AI/phase3_ses/artifacts_handover/handover_table.csv\n","Stress index stats:\n","count    25168.000000\n","mean         0.116191\n","std          0.082112\n","min          0.004004\n","25%          0.043627\n","50%          0.111218\n","75%          0.165828\n","max          0.494763\n","Name: stress_index, dtype: float64\n","Saved: /content/drive/MyDrive/Colab_Notebooks/Thesis-AI/phase3_ses/artifacts_stress/stress_timeseries.csv\n","Saved: /content/drive/MyDrive/Colab_Notebooks/Thesis-AI/phase3_ses/artifacts_stress/stress_top_windows.csv\n","Saved: /content/drive/MyDrive/Colab_Notebooks/Thesis-AI/phase3_ses/artifacts_stress/stress_global_means.csv\n","\n","--- DONE: Constellation Stress Index built. ---\n"]}],"source":["####################################################################################################\n","# SES Prototype – Constellation Stress Index & Joint Risk Radar\n","#\n","# This notebook builds a synthetic \"stress index\" timeseries by combining:\n","#  - Capacity risk per beam (from Risk-aware Capacity Advisor)\n","#  - Jamming / interference anomaly score\n","#  - SLA breach windows\n","#  - Signal loss windows\n","#  - Beam handover anomalies\n","#\n","# It exports:\n","#  - artifacts_stress/stress_timeseries.csv\n","#  - artifacts_stress/stress_top_windows.csv\n","####################################################################################################\n","\n","import os\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","\n","# -------------------------------------------------------------------\n","# Step 1 – Mount Google Drive and set paths\n","# -------------------------------------------------------------------\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","\n","BASE_DIR = Path(\"/content/drive/MyDrive/Colab_Notebooks/Thesis-AI/phase3_ses\")\n","ART_CAP = BASE_DIR / \"artifacts_capacity\"\n","ART_SLA = BASE_DIR / \"artifacts_sla\"\n","ART_SIG = BASE_DIR / \"artifacts_signal_loss\"\n","ART_HAND = BASE_DIR / \"artifacts_handover\"\n","ART_STRESS = BASE_DIR / \"artifacts_stress\"\n","\n","ART_STRESS.mkdir(exist_ok=True)\n","\n","print(\"BASE_DIR    :\", BASE_DIR)\n","print(\"ART_CAP     :\", ART_CAP)\n","print(\"ART_SLA     :\", ART_SLA)\n","print(\"ART_SIG     :\", ART_SIG)\n","print(\"ART_HAND    :\", ART_HAND)\n","print(\"ART_STRESS  :\", ART_STRESS)\n","\n","# -------------------------------------------------------------------\n","# Step 2 – Helper functions\n","# -------------------------------------------------------------------\n","def load_csv(path, parse_dates=None):\n","    if path.exists():\n","        print(f\"Loading: {path}\")\n","        return pd.read_csv(path, parse_dates=parse_dates)\n","    else:\n","        print(f\"[WARN] Missing file: {path}\")\n","        return None\n","\n","def min_max_norm(series):\n","    s = series.astype(float)\n","    mn, mx = s.min(), s.max()\n","    if np.isfinite(mn) and np.isfinite(mx) and mx > mn:\n","        return (s - mn) / (mx - mn)\n","    else:\n","        return pd.Series(0.0, index=s.index)\n","\n","\n","def indicator_from_intervals(times, intervals, pad_before=\"0min\", pad_after=\"0min\"):\n","    \"\"\"\n","    times: DatetimeIndex\n","    intervals: list of (start, end) timestamps\n","    returns Series of 0/1 indicating stress around those intervals.\n","    \"\"\"\n","    out = np.zeros(len(times), dtype=float)\n","    pad_before = pd.to_timedelta(pad_before)\n","    pad_after = pd.to_timedelta(pad_after)\n","    for (s, e) in intervals:\n","        if pd.isna(s) or pd.isna(e):\n","            continue\n","        s_pad = s - pad_before\n","        e_pad = e + pad_after\n","        mask = (times >= s_pad) & (times <= e_pad)\n","        out[mask] = 1.0\n","    return pd.Series(out, index=times)\n","\n","\n","# -------------------------------------------------------------------\n","# Step 3 – Load capacity-risk timeseries (base grid)\n","# -------------------------------------------------------------------\n","# From the Risk-aware Capacity Advisor notebook we expect:\n","# artifacts_capacity/capacity_risk_timeseries.csv\n","# with columns: time, beam_id, demand_forecast_mbps, capacity_mbps, risk_score\n","\n","df_cap = load_csv(ART_CAP / \"capacity_risk_timeseries.csv\", parse_dates=[\"time\"])\n","if df_cap is None:\n","    raise RuntimeError(\n","        \"Capacity risk dataset not found. \"\n","        \"Run the Risk-aware Capacity Advisor notebook first \"\n","        \"so it saves artifacts_capacity/capacity_risk_timeseries.csv\"\n","    )\n","\n","df_cap = df_cap.copy()\n","df_cap[\"time\"] = pd.to_datetime(df_cap[\"time\"], utc=True)\n","df_cap = df_cap.sort_values([\"beam_id\", \"time\"])\n","print(\"Capacity risk shape:\", df_cap.shape)\n","print(df_cap.head())\n","\n","# Base grid for stress index: every (time, beam_id) present in df_cap\n","times = df_cap[\"time\"]\n","beams = df_cap[\"beam_id\"].unique()\n","print(\"Beams in capacity dataset:\", beams)\n","\n","# -------------------------------------------------------------------\n","# Step 4 – Capacity component (already 0–1 risk_score)\n","# -------------------------------------------------------------------\n","# The column name 'risk_score' was not found in df_cap. Using 'risk_index' instead.\n","if \"risk_index\" not in df_cap.columns:\n","    raise RuntimeError(\"capacity_risk_timeseries.csv must have a 'risk_index' column.\")\n","cap_risk = min_max_norm(df_cap[\"risk_index\"])\n","df_cap[\"capacity_risk\"] = cap_risk\n","\n","# -------------------------------------------------------------------\n","# Step 5 – Jamming / interference component (global anomaly score)\n","# -------------------------------------------------------------------\n","# We use ses_comm_anomalies.csv anomaly_score and broadcast to beams.\n","COMM_ANOM = BASE_DIR / \"ses_comm_anomalies.csv\"\n","df_comm = load_csv(COMM_ANOM, parse_dates=[\"time\"])\n","\n","if df_comm is not None and \"anomaly_score\" in df_comm.columns:\n","    df_comm = df_comm.copy()\n","    df_comm[\"time\"] = pd.to_datetime(df_comm[\"time\"], utc=True)\n","    df_comm = df_comm.sort_values(\"time\")\n","    jam = df_comm.set_index(\"time\")[\"anomaly_score\"].astype(float)\n","    # Reindex to our grid times, interpolate, forward-fill, then min-max normalise\n","    jam_on_grid = jam.reindex(times).interpolate().ffill().fillna(0.0)\n","    jam_risk = min_max_norm(jam_on_grid)\n","else:\n","    print(\"[INFO] No jamming anomalies found – setting jamming_risk = 0.\")\n","    jam_risk = pd.Series(0.0, index=df_cap.index)\n","\n","# because df_cap has multiple beams per time, we need same length; times is same length as df_cap\n","df_cap[\"jamming_risk\"] = jam_risk.values\n","\n","# -------------------------------------------------------------------\n","# Step 6 – SLA breach component (interval-based)\n","# -------------------------------------------------------------------\n","SLA_EVENTS = ART_SLA / \"sla_breach_events.csv\"\n","df_sla = load_csv(SLA_EVENTS, parse_dates=[\"start\", \"end\"])\n","\n","if df_sla is not None and not df_sla.empty:\n","    df_sla = df_sla.copy()\n","    df_sla[\"start\"] = pd.to_datetime(df_sla[\"start\"], utc=True, errors=\"coerce\")\n","    df_sla[\"end\"] = pd.to_datetime(df_sla[\"end\"], utc=True, errors=\"coerce\")\n","    intervals_sla = list(zip(df_sla[\"start\"], df_sla[\"end\"]))\n","    # pad a bit before/after each breach to reflect early-warning and lingering impact\n","    sla_indicator = indicator_from_intervals(times, intervals_sla,\n","                                             pad_before=\"5min\", pad_after=\"10min\")\n","else:\n","    print(\"[INFO] No SLA breach events found – sla_risk = 0.\")\n","    sla_indicator = pd.Series(0.0, index=times)\n","\n","# repeat over beams (same value for each beam at a given time)\n","df_cap[\"sla_risk\"] = sla_indicator.values\n","\n","# -------------------------------------------------------------------\n","# Step 7 – Signal-loss component (interval-based)\n","# -------------------------------------------------------------------\n","SIG_EVENTS = ART_SIG / \"signal_loss_events.csv\"\n","df_sig = load_csv(SIG_EVENTS, parse_dates=[\"start_time\", \"end_time\"])\n","\n","if df_sig is not None and not df_sig.empty:\n","    df_sig = df_sig.copy()\n","    start_col, end_col = None, None\n","    for c in df_sig.columns:\n","        lc = c.lower()\n","        if \"start\" in lc and start_col is None:\n","            start_col = c\n","        if \"end\" in lc and end_col is None:\n","            end_col = c\n","    if start_col is not None and end_col is not None:\n","        df_sig[start_col] = pd.to_datetime(df_sig[start_col], utc=True, errors=\"coerce\")\n","        df_sig[end_col] = pd.to_datetime(df_sig[end_col], utc=True, errors=\"coerce\")\n","        intervals_sig = list(zip(df_sig[start_col], df_sig[end_col]))\n","        loss_indicator = indicator_from_intervals(times, intervals_sig,\n","                                                  pad_before=\"2min\", pad_after=\"5min\")\n","    else:\n","        print(\"[WARN] Could not find start/end columns in signal-loss events – loss_risk = 0.\")\n","        loss_indicator = pd.Series(0.0, index=times)\n","else:\n","    print(\"[INFO] No signal-loss events found – loss_risk = 0.\")\n","    loss_indicator = pd.Series(0.0, index=times)\n","\n","df_cap[\"loss_risk\"] = loss_indicator.values\n","\n","# -------------------------------------------------------------------\n","# Step 8 – Beam-handover component (point events)\n","# -------------------------------------------------------------------\n","HAND_TABLE = ART_HAND / \"handover_table.csv\"\n","df_hand = load_csv(HAND_TABLE, parse_dates=[\"t\"])\n","\n","if df_hand is not None and not df_hand.empty:\n","    df_hand = df_hand.copy()\n","    if \"t\" in df_hand.columns:\n","        df_hand[\"t\"] = pd.to_datetime(df_hand[\"t\"], utc=True, errors=\"coerce\")\n","    else:\n","        # fallback: try first datetime-like column\n","        dt_col = None\n","        for c in df_hand.columns:\n","            if \"time\" in c.lower() or \"t\" == c.lower():\n","                dt_col = c\n","                break\n","        if dt_col is None:\n","            raise RuntimeError(\"handover_table.csv has no time column.\")\n","        df_hand[\"t\"] = pd.to_datetime(df_hand[dt_col], utc=True, errors=\"coerce\")\n","\n","    # treat anomalous == 1 as risky; if no such col, treat all as risky\n","    if \"anomalous\" in df_hand.columns:\n","        df_hand = df_hand[df_hand[\"anomalous\"] == 1]\n","\n","    intervals_hand = list(zip(df_hand[\"t\"], df_hand[\"t\"]))\n","    hand_indicator = indicator_from_intervals(times, intervals_hand,\n","                                              pad_before=\"2min\", pad_after=\"2min\")\n","else:\n","    print(\"[INFO] No beam handover anomalies found – handover_risk = 0.\")\n","    hand_indicator = pd.Series(0.0, index=times)\n","\n","df_cap[\"handover_risk\"] = hand_indicator.values\n","\n","# -------------------------------------------------------------------\n","# Step 9 – Compute stress index as weighted combination\n","# -------------------------------------------------------------------\n","# All components are in [0,1] (or 0/1). We now define a simple\n","# explainable linear formula (your \"invention\" – can be tuned later).\n","\n","W_CAP = 0.40  # capacity risk is main driver\n","W_JAM = 0.20  # interference matters a lot\n","W_SLA = 0.15  # SLA breaches are important\n","W_LOSS = 0.15  # direct signal loss\n","W_HAND = 0.10  # handover risk\n","\n","df_cap[\"stress_index_raw\"] = (\n","    W_CAP * df_cap[\"capacity_risk\"]\n","    + W_JAM * df_cap[\"jamming_risk\"]\n","    + W_SLA * df_cap[\"sla_risk\"]\n","    + W_LOSS * df_cap[\"loss_risk\"]\n","    + W_HAND * df_cap[\"handover_risk\"]\n",")\n","\n","df_cap[\"stress_index\"] = df_cap[\"stress_index_raw\"].clip(0, 1)\n","\n","print(\"Stress index stats:\")\n","print(df_cap[\"stress_index\"].describe())\n","\n","# -------------------------------------------------------------------\n","# Step 10 – Export stress timeseries & top windows\n","# -------------------------------------------------------------------\n","cols_out = [\n","    \"time\",\n","    \"beam_id\",\n","    \"capacity_risk\",\n","    \"jamming_risk\",\n","    \"sla_risk\",\n","    \"loss_risk\",\n","    \"handover_risk\",\n","    \"stress_index\",\n","]\n","\n","df_stress = df_cap[cols_out].copy()\n","df_stress.to_csv(ART_STRESS / \"stress_timeseries.csv\", index=False)\n","print(\"Saved:\", ART_STRESS / \"stress_timeseries.csv\")\n","\n","# Top windows per beam\n","df_top = (\n","    df_stress.sort_values(\"stress_index\", ascending=False)\n","    .groupby(\"beam_id\")\n","    .head(10)\n","    .reset_index(drop=True)\n",")\n","df_top.to_csv(ART_STRESS / \"stress_top_windows.csv\", index=False)\n","print(\"Saved:\", ART_STRESS / \"stress_top_windows.csv\")\n","\n","# Simple global averages (can be used as a \"feature importance\" approximation)\n","df_contrib = df_stress[[\n","    \"capacity_risk\",\n","    \"jamming_risk\",\n","    \"sla_risk\",\n","    \"loss_risk\",\n","    \"handover_risk\",\n","    \"stress_index\",\n","]].mean().to_frame(name=\"global_mean\").reset_index().rename(columns={\"index\": \"component\"})\n","df_contrib.to_csv(ART_STRESS / \"stress_global_means.csv\", index=False)\n","print(\"Saved:\", ART_STRESS / \"stress_global_means.csv\")\n","\n","print(\"\\n--- DONE: Constellation Stress Index built. ---\")"]}]}